<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.251">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Ramdayal Rewaria">

<ul style="list-style-type: none; margin: 0; padding: 0; overflow: hidden; background-color: red ; max-width:100%; padding-top:0%; padding-bottom:0%; display: flex; justify-content: center; font-size:small;">
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="../index.html">About Me</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="https://github.com/anly501/anly-501-project-rdr-190100">Code</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="https://github.com/anly501/anly-501-project-rdr-190100/tree/main/data">Data</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./introductions.html">Introduction</a>
  </li>
  <li class="dropdwn" style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="javascript:void(0)" class="drpbtn">Data Gathering</a>
    <div class="dropdwn-content">
      <a style = "display: block;" href="./data_gathering_twitterdata.html">Twitter Data</a>
      <a style = "display: block;" href="./data_gathering_recorddata.html">Record Data</a>
    </div>
  </li>
  <li class="dropdwn" style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="javascript:void(0)" class="drpbtn">Data Cleaning</a>
    <div class="dropdwn-content">
      <a style = "display: block;" href="./data_cleaning_twitterdata.html">Twitter Data</a>
      <a style = "display: block;" href="./data_cleaning_recorddata1.html">Record Data Part 1 in R</a>
      <a style = "display: block;" href="./data_cleaning_recorddata2.html">Record Data Part 2 in Python</a>
    </div>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./data_exploration.html">Exploring Data</a>
  </li>
  <li class="dropdwn" style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="javascript:void(0)" class="drpbtn">Naive Bayes’</a>
    <div class="dropdwn-content">
      <a style = "display: block;" href="./Naive-Bayes-twitterdata.html">Twitter Data in Python</a>
      <a style = "display: block;" href="./Naive-Bayes-recorddata.html">Record Data in R</a>
    </div>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./Decision-Trees.html">Decision Trees</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./SVM-Record-data.html">SVM</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./Clustering.html">Clustering</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./arm_and_networking.html">ARM and Networking</a>
  </li>
  <li style = "display: inline-block; color: white; text-align: center; padding: 6px 6px;">
    <a style = "display: inline-block; color: white; text-align: center; padding: 6px 6px; text-decoration: none; outline-style: solid; outline-color: black; margin-left: -2px;" 
    href="./conclusions.html">Conclusion</a>
  </li>
  </ul>

  <video width="100%" height="100%" autoplay muted>
    <source src="./f1_video1.mp4" type="video/mp4"/>
  </video>

<title>Support Vector Machine Classifier for Record Data</title>

<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */

li a:hover, .dropdwn:hover .drpbtn {
  background-color: black;
}
.dropdwn-content {
    display: none;
    position:static;
    background-color: black;
    min-width: 160px; 
}
.dropdwn-content a {
    color: white;
    padding: 12px 16px;
    text-decoration: none;
    display: block;
    text-align: center;
}
.dropdwn:hover .dropdwn-content {
  display: block;
  opacity: 1; 
}
li.dropdwn {
  display: inline-block;
}
</style>


<script src="SVM-Record-data_files/libs/clipboard/clipboard.min.js"></script>
<script src="SVM-Record-data_files/libs/quarto-html/quarto.js"></script>
<script src="SVM-Record-data_files/libs/quarto-html/popper.min.js"></script>
<script src="SVM-Record-data_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="SVM-Record-data_files/libs/quarto-html/anchor.min.js"></script>
<link href="SVM-Record-data_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="SVM-Record-data_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="SVM-Record-data_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="SVM-Record-data_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="SVM-Record-data_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="dark">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Support Vector Machine Classifier for Record Data</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Ramdayal Rewaria </p>
          </div>
  </div>
    
    
  </div>
  

</header>

<section id="import-libraries" class="level1">
<h1>Import Libraries</h1>
<div class="cell" data-execution_count="100">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy <span class="im">import</span> stats</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, ConfusionMatrixDisplay</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split, GridSearchCV</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.svm <span class="im">import</span> SVC</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> classification_report</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> OneHotEncoder, StandardScaler</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> confusion_matrix, plot_confusion_matrix,<span class="op">\</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>    precision_score, recall_score, accuracy_score, f1_score, log_loss,<span class="op">\</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>    roc_curve, roc_auc_score, classification_report</span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.pipeline <span class="im">import</span> Pipeline, FeatureUnion, make_pipeline</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.compose <span class="im">import</span> ColumnTransformer</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> warnings </span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>warnings.filterwarnings(<span class="st">"ignore"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="import-data" class="level1">
<h1>Import Data</h1>
<ul>
<li>Cleaned record data is considered to perform SVM supervised learning algorithm and predict the label variable (Podium, Top 10 or Outside Top 10).</li>
<li>The data consists of 26,941 rows and 22 feature variables and 1 label column.</li>
<li>It is a historical record data of all the races that have happened in the past 71 years with the results of every position that a driver has held in all the races.</li>
<li>Some of the feature variables include laps in the race, grid position held, age at time of the race, history of wins in the past, history of laps completed in the past, weather of the race, points gained in the race and many more.</li>
</ul>
<div class="cell" data-execution_count="78">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/02-model-data/data_cleaned.csv'</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="78">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>season</th>
      <th>round</th>
      <th>season_round</th>
      <th>driverId</th>
      <th>raceId</th>
      <th>circuitId</th>
      <th>position</th>
      <th>points</th>
      <th>grid</th>
      <th>laps</th>
      <th>...</th>
      <th>weather</th>
      <th>stop</th>
      <th>age_on_race</th>
      <th>cumulative_points</th>
      <th>cumulative_laps</th>
      <th>pole_driverId</th>
      <th>pole_history</th>
      <th>win_driverId</th>
      <th>win_history</th>
      <th>label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1950</td>
      <td>1</td>
      <td>1950_1</td>
      <td>642</td>
      <td>833</td>
      <td>9</td>
      <td>1</td>
      <td>9.0</td>
      <td>1</td>
      <td>70</td>
      <td>...</td>
      <td>Fine</td>
      <td>Not Available</td>
      <td>44</td>
      <td>9.0</td>
      <td>70</td>
      <td>642</td>
      <td>1</td>
      <td>642</td>
      <td>1</td>
      <td>Podium</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1950</td>
      <td>1</td>
      <td>1950_1</td>
      <td>786</td>
      <td>833</td>
      <td>9</td>
      <td>2</td>
      <td>6.0</td>
      <td>2</td>
      <td>70</td>
      <td>...</td>
      <td>Fine</td>
      <td>Not Available</td>
      <td>52</td>
      <td>6.0</td>
      <td>70</td>
      <td>642</td>
      <td>0</td>
      <td>642</td>
      <td>0</td>
      <td>Podium</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1950</td>
      <td>1</td>
      <td>1950_1</td>
      <td>686</td>
      <td>833</td>
      <td>9</td>
      <td>3</td>
      <td>4.0</td>
      <td>4</td>
      <td>70</td>
      <td>...</td>
      <td>Fine</td>
      <td>Not Available</td>
      <td>39</td>
      <td>4.0</td>
      <td>70</td>
      <td>642</td>
      <td>0</td>
      <td>642</td>
      <td>0</td>
      <td>Podium</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1950</td>
      <td>1</td>
      <td>1950_1</td>
      <td>704</td>
      <td>833</td>
      <td>9</td>
      <td>4</td>
      <td>3.0</td>
      <td>6</td>
      <td>68</td>
      <td>...</td>
      <td>Fine</td>
      <td>Not Available</td>
      <td>46</td>
      <td>3.0</td>
      <td>68</td>
      <td>642</td>
      <td>0</td>
      <td>642</td>
      <td>0</td>
      <td>Top_10</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1950</td>
      <td>1</td>
      <td>1950_1</td>
      <td>627</td>
      <td>833</td>
      <td>9</td>
      <td>5</td>
      <td>2.0</td>
      <td>9</td>
      <td>68</td>
      <td>...</td>
      <td>Fine</td>
      <td>Not Available</td>
      <td>45</td>
      <td>2.0</td>
      <td>68</td>
      <td>642</td>
      <td>0</td>
      <td>642</td>
      <td>0</td>
      <td>Top_10</td>
    </tr>
  </tbody>
</table>
<p>5 rows × 22 columns</p>
</div>
</div>
</div>
<div class="cell" data-execution_count="79">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>driver_df <span class="op">=</span> pd.read_csv(<span class="st">'../../data/00-raw-data/drivers.csv'</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>driver_df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="79">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>driverId</th>
      <th>driverRef</th>
      <th>number</th>
      <th>code</th>
      <th>forename</th>
      <th>surname</th>
      <th>dob</th>
      <th>nationality</th>
      <th>url</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1</td>
      <td>hamilton</td>
      <td>44</td>
      <td>HAM</td>
      <td>Lewis</td>
      <td>Hamilton</td>
      <td>1985-01-07</td>
      <td>British</td>
      <td>http://en.wikipedia.org/wiki/Lewis_Hamilton</td>
    </tr>
    <tr>
      <th>1</th>
      <td>2</td>
      <td>heidfeld</td>
      <td>\N</td>
      <td>HEI</td>
      <td>Nick</td>
      <td>Heidfeld</td>
      <td>1977-05-10</td>
      <td>German</td>
      <td>http://en.wikipedia.org/wiki/Nick_Heidfeld</td>
    </tr>
    <tr>
      <th>2</th>
      <td>3</td>
      <td>rosberg</td>
      <td>6</td>
      <td>ROS</td>
      <td>Nico</td>
      <td>Rosberg</td>
      <td>1985-06-27</td>
      <td>German</td>
      <td>http://en.wikipedia.org/wiki/Nico_Rosberg</td>
    </tr>
    <tr>
      <th>3</th>
      <td>4</td>
      <td>alonso</td>
      <td>14</td>
      <td>ALO</td>
      <td>Fernando</td>
      <td>Alonso</td>
      <td>1981-07-29</td>
      <td>Spanish</td>
      <td>http://en.wikipedia.org/wiki/Fernando_Alonso</td>
    </tr>
    <tr>
      <th>4</th>
      <td>5</td>
      <td>kovalainen</td>
      <td>\N</td>
      <td>KOV</td>
      <td>Heikki</td>
      <td>Kovalainen</td>
      <td>1981-10-19</td>
      <td>Finnish</td>
      <td>http://en.wikipedia.org/wiki/Heikki_Kovalainen</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="80">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.merge(df, driver_df[[<span class="st">'driverId'</span>, <span class="st">'driverRef'</span>]], on<span class="op">=</span><span class="st">'driverId'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="81">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>df.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="81">
<pre><code>(26941, 23)</code></pre>
</div>
</div>
</section>
<section id="data-pre-processing-and-visualization" class="level1">
<h1>Data Pre-Processing and Visualization</h1>
<ul>
<li>The data was cleaned in the sections before but there are still some pre-processing left to be in order for the data to be “model-ready”.</li>
<li>Some unnecessary columns are dropped and columns are segregated into numeric and categorical sections.</li>
<li>The numerical columns are scaled using a Standard Scaler and the categorical columns are one hot encoded to minimize loss of data. All of this is done with the help of a function which use sklearn’s Pipeline module.</li>
<li>If a transformer and model estimator are applied separately, it will result in fitted training features being wrongly included in the test-fold of GridSearchCV.</li>
<li>Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.</li>
<li>If you separate feature scaling and model-fitting functions while using GridSearchCV, you will be creating a biased testing dataset that already contains information about the training set which is not good.</li>
<li>Furthermore, the data is split into training and testing but not traditionally (with the help of sklearn’s train_test_split). The training set is made up of races before 2021 and the testing is done on the races of 2021.</li>
</ul>
<div class="cell" data-execution_count="82">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>df.drop([<span class="st">'season_round'</span>, <span class="st">'constructorRef'</span>, <span class="st">'raceId'</span>, <span class="st">'driverId'</span>], axis<span class="op">=</span><span class="dv">1</span>, inplace<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="83">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>df.info()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>&lt;class 'pandas.core.frame.DataFrame'&gt;
Int64Index: 26941 entries, 0 to 26940
Data columns (total 19 columns):
 #   Column             Non-Null Count  Dtype  
---  ------             --------------  -----  
 0   season             26941 non-null  int64  
 1   round              26941 non-null  int64  
 2   circuitId          26941 non-null  int64  
 3   position           26941 non-null  int64  
 4   points             26941 non-null  float64
 5   grid               26941 non-null  int64  
 6   laps               26941 non-null  int64  
 7   status             26941 non-null  object 
 8   weather            26941 non-null  object 
 9   stop               26941 non-null  object 
 10  age_on_race        26941 non-null  int64  
 11  cumulative_points  26941 non-null  float64
 12  cumulative_laps    26941 non-null  int64  
 13  pole_driverId      26941 non-null  int64  
 14  pole_history       26941 non-null  int64  
 15  win_driverId       26941 non-null  int64  
 16  win_history        26941 non-null  int64  
 17  label              26941 non-null  object 
 18  driverRef          26941 non-null  object 
dtypes: float64(2), int64(12), object(5)
memory usage: 4.1+ MB</code></pre>
</div>
</div>
<div class="cell" data-execution_count="84">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>df.head()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="84">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>season</th>
      <th>round</th>
      <th>circuitId</th>
      <th>position</th>
      <th>points</th>
      <th>grid</th>
      <th>laps</th>
      <th>status</th>
      <th>weather</th>
      <th>stop</th>
      <th>age_on_race</th>
      <th>cumulative_points</th>
      <th>cumulative_laps</th>
      <th>pole_driverId</th>
      <th>pole_history</th>
      <th>win_driverId</th>
      <th>win_history</th>
      <th>label</th>
      <th>driverRef</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>1950</td>
      <td>1</td>
      <td>9</td>
      <td>1</td>
      <td>9.0</td>
      <td>1</td>
      <td>70</td>
      <td>Finished</td>
      <td>Fine</td>
      <td>Not Available</td>
      <td>44</td>
      <td>9.0</td>
      <td>70</td>
      <td>642</td>
      <td>1</td>
      <td>642</td>
      <td>1</td>
      <td>Podium</td>
      <td>farina</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1950</td>
      <td>2</td>
      <td>6</td>
      <td>11</td>
      <td>0.0</td>
      <td>2</td>
      <td>0</td>
      <td>Accident</td>
      <td>Not Available</td>
      <td>Not Available</td>
      <td>44</td>
      <td>9.0</td>
      <td>70</td>
      <td>579</td>
      <td>1</td>
      <td>579</td>
      <td>1</td>
      <td>Outside_Top_10</td>
      <td>farina</td>
    </tr>
    <tr>
      <th>2</th>
      <td>1950</td>
      <td>4</td>
      <td>66</td>
      <td>1</td>
      <td>9.0</td>
      <td>2</td>
      <td>42</td>
      <td>Finished</td>
      <td>Sunny</td>
      <td>Not Available</td>
      <td>44</td>
      <td>18.0</td>
      <td>112</td>
      <td>579</td>
      <td>1</td>
      <td>642</td>
      <td>2</td>
      <td>Podium</td>
      <td>farina</td>
    </tr>
    <tr>
      <th>3</th>
      <td>1950</td>
      <td>5</td>
      <td>13</td>
      <td>4</td>
      <td>4.0</td>
      <td>1</td>
      <td>35</td>
      <td>Finished</td>
      <td>Sunny</td>
      <td>Not Available</td>
      <td>44</td>
      <td>22.0</td>
      <td>147</td>
      <td>642</td>
      <td>2</td>
      <td>579</td>
      <td>2</td>
      <td>Top_10</td>
      <td>farina</td>
    </tr>
    <tr>
      <th>4</th>
      <td>1950</td>
      <td>6</td>
      <td>55</td>
      <td>7</td>
      <td>0.0</td>
      <td>2</td>
      <td>55</td>
      <td>Mechanical_Issue</td>
      <td>Sunny</td>
      <td>Not Available</td>
      <td>44</td>
      <td>22.0</td>
      <td>202</td>
      <td>579</td>
      <td>2</td>
      <td>579</td>
      <td>2</td>
      <td>Top_10</td>
      <td>farina</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<div class="cell" data-execution_count="87">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[df[<span class="st">'season'</span>] <span class="op">!=</span> <span class="dv">2022</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Splitting data into train and test:</p>
<div class="cell" data-execution_count="88">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> df[df[<span class="st">'season'</span>] <span class="op">!=</span> <span class="dv">2021</span>].drop(columns <span class="op">=</span> [<span class="st">'label'</span>])</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> df.loc[df[<span class="st">'season'</span>] <span class="op">!=</span> <span class="dv">2021</span>, [<span class="st">'season'</span>, <span class="st">'round'</span>, <span class="st">'driverRef'</span>, <span class="st">'label'</span>]]</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> df[df[<span class="st">'season'</span>] <span class="op">==</span> <span class="dv">2021</span>].drop(columns <span class="op">=</span> [<span class="st">'label'</span>])</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> df.loc[(df[<span class="st">'season'</span>] <span class="op">==</span> <span class="dv">2021</span>), [<span class="st">'season'</span>, <span class="st">'round'</span>, <span class="st">'driverRef'</span>, <span class="st">'label'</span>]]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Grouping the data by setting the index of train and test data into season, round and driver references:</p>
<div class="cell" data-execution_count="91">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> X_train.set_index([<span class="st">'season'</span>, <span class="st">'round'</span>, <span class="st">'driverRef'</span>])</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>y_train <span class="op">=</span> y_train.set_index([<span class="st">'season'</span>, <span class="st">'round'</span>, <span class="st">'driverRef'</span>])</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> X_test.set_index([<span class="st">'season'</span>, <span class="st">'round'</span>, <span class="st">'driverRef'</span>])</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>y_test <span class="op">=</span> y_test.set_index([<span class="st">'season'</span>, <span class="st">'round'</span>, <span class="st">'driverRef'</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="94">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>numeric_features <span class="op">=</span> [<span class="st">'circuitId'</span>, <span class="st">'position'</span>, <span class="st">'points'</span>, <span class="st">'grid'</span>, <span class="st">'laps'</span>, <span class="st">'age_on_race'</span>, <span class="st">'cumulative_points'</span>, <span class="st">'cumulative_laps'</span>,</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>       <span class="st">'pole_driverId'</span>, <span class="st">'pole_history'</span>, <span class="st">'win_driverId'</span>, <span class="st">'win_history'</span>]</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>categorical_features <span class="op">=</span> [<span class="st">'status'</span>, <span class="st">'weather'</span>, <span class="st">'stop'</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="95">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>display(X_test.head())</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>display(y_test.head())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th>circuitId</th>
      <th>position</th>
      <th>points</th>
      <th>grid</th>
      <th>laps</th>
      <th>status</th>
      <th>weather</th>
      <th>stop</th>
      <th>age_on_race</th>
      <th>cumulative_points</th>
      <th>cumulative_laps</th>
      <th>pole_driverId</th>
      <th>pole_history</th>
      <th>win_driverId</th>
      <th>win_history</th>
    </tr>
    <tr>
      <th>season</th>
      <th>round</th>
      <th>driverRef</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">2021</th>
      <th>1</th>
      <th>raikkonen</th>
      <td>3</td>
      <td>11</td>
      <td>0.0</td>
      <td>14</td>
      <td>56</td>
      <td>Finished</td>
      <td>Sunny</td>
      <td>Two</td>
      <td>42</td>
      <td>1863.0</td>
      <td>17613</td>
      <td>830</td>
      <td>18</td>
      <td>1</td>
      <td>21</td>
    </tr>
    <tr>
      <th>2</th>
      <th>raikkonen</th>
      <td>21</td>
      <td>13</td>
      <td>0.0</td>
      <td>16</td>
      <td>63</td>
      <td>Finished</td>
      <td>Rainy</td>
      <td>Three</td>
      <td>42</td>
      <td>1863.0</td>
      <td>17676</td>
      <td>1</td>
      <td>18</td>
      <td>830</td>
      <td>21</td>
    </tr>
    <tr>
      <th>3</th>
      <th>raikkonen</th>
      <td>75</td>
      <td>20</td>
      <td>0.0</td>
      <td>15</td>
      <td>1</td>
      <td>Mechanical_Issue</td>
      <td>Cloudy</td>
      <td>Not Available</td>
      <td>42</td>
      <td>1863.0</td>
      <td>17677</td>
      <td>822</td>
      <td>18</td>
      <td>1</td>
      <td>21</td>
    </tr>
    <tr>
      <th>4</th>
      <th>raikkonen</th>
      <td>4</td>
      <td>12</td>
      <td>0.0</td>
      <td>17</td>
      <td>65</td>
      <td>Lapped</td>
      <td>Cloudy</td>
      <td>One</td>
      <td>42</td>
      <td>1863.0</td>
      <td>17742</td>
      <td>1</td>
      <td>18</td>
      <td>1</td>
      <td>21</td>
    </tr>
    <tr>
      <th>5</th>
      <th>raikkonen</th>
      <td>6</td>
      <td>11</td>
      <td>0.0</td>
      <td>14</td>
      <td>77</td>
      <td>Lapped</td>
      <td>Sunny</td>
      <td>One</td>
      <td>42</td>
      <td>1863.0</td>
      <td>17819</td>
      <td>844</td>
      <td>18</td>
      <td>830</td>
      <td>21</td>
    </tr>
  </tbody>
</table>
</div>
</div>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th>label</th>
    </tr>
    <tr>
      <th>season</th>
      <th>round</th>
      <th>driverRef</th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="5" valign="top">2021</th>
      <th>1</th>
      <th>raikkonen</th>
      <td>Outside_Top_10</td>
    </tr>
    <tr>
      <th>2</th>
      <th>raikkonen</th>
      <td>Outside_Top_10</td>
    </tr>
    <tr>
      <th>3</th>
      <th>raikkonen</th>
      <td>Outside_Top_10</td>
    </tr>
    <tr>
      <th>4</th>
      <th>raikkonen</th>
      <td>Outside_Top_10</td>
    </tr>
    <tr>
      <th>5</th>
      <th>raikkonen</th>
      <td>Outside_Top_10</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>Creating a function with sklearn’s Pipeline module and transformers to convert categorical and numerical features:</p>
<div class="cell" data-execution_count="98">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> prediction_model(model_type, model_id):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Scale numeric features using 'StandardScaler' and 'One-Hot Encode' categorical features</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    scoring <span class="op">=</span> [<span class="st">'neg_log_loss'</span>, <span class="st">'accuracy'</span>]</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>    numeric_transformer <span class="op">=</span> Pipeline(steps<span class="op">=</span>[(<span class="st">'scaler'</span>, StandardScaler())])</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>    categorical_transformer <span class="op">=</span> Pipeline(steps<span class="op">=</span>[(<span class="st">'ohe'</span>, OneHotEncoder(handle_unknown <span class="op">=</span> <span class="st">'ignore'</span>))])</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>    preprocessor <span class="op">=</span> ColumnTransformer(transformers<span class="op">=</span>[(<span class="st">'num'</span>, numeric_transformer, numeric_features),</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>                                                   (<span class="st">'cat'</span>, categorical_transformer, categorical_features)])</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>    pipeline <span class="op">=</span> Pipeline(steps<span class="op">=</span>[(<span class="st">'prep'</span>, preprocessor), </span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>                               (model_id, model_type)])</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pipeline</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="svm" class="level1">
<h1>SVM</h1>
<ul>
<li><code>Support Vector Machines</code> are a set of supervised learning methods used for classification, regression, and outliers detection. All of these are common tasks in machine learning.</li>
<li>There are specific types of SVMs you can use for particular machine learning problems, like support vector regression (SVR) which is an extension of support vector classification (SVC).</li>
<li>SVMs are different from other classification algorithms because of the way they choose the decision boundary that maximizes the distance from the nearest data points of all the classes. The decision boundary created by SVMs is called the maximum margin classifier or the maximum margin hyper plane.</li>
<li>A simple linear SVM classifier works by making a straight line between two classes. That means all of the data points on one side of the line will represent a category and the data points on the other side of the line will be put into a different category. This means there can be an infinite number of lines to choose from.</li>
<li>What makes the linear SVM algorithm better than some of the other algorithms, like k-nearest neighbors, is that it chooses the best line to classify your data points. It chooses the line that separates the data and is the furthest away from the closet data points as possible.</li>
<li>Pros
<ul>
<li>Effective on datasets with multiple features, like financial or medical data.</li>
<li>Uses a subset of training points in the decision function called support vectors which makes it memory efficient.</li>
<li>Different kernel functions can be specified for the decision function. You can use common kernels, but it’s also possible to specify custom kernels.</li>
</ul></li>
<li>Cons
<ul>
<li>If the number of features is a lot bigger than the number of data points, avoiding over-fitting when choosing kernel functions and regularization term is crucial.</li>
<li>SVMs don’t directly provide probability estimates. Those are calculated using an expensive five-fold cross-validation.</li>
<li>Works best on small sample sets because of its high training time.</li>
</ul></li>
</ul>
<section id="model-prediction-function" class="level2">
<h2 class="anchored" data-anchor-id="model-prediction-function">Model Prediction function</h2>
<ul>
<li>After fitting the model it is important to showcase and visualize the model classification results.</li>
<li>The model_results function predicts the model results on test data (2021 races). It displays out the 40 results from the test data along with the prediction probabilities.</li>
<li>The function also fills the prediction scorecard dictionary which contains:
<ol type="1">
<li>Model</li>
<li>Accuracy</li>
<li>Precision</li>
<li>Recall</li>
<li>Best parameters</li>
</ol></li>
</ul>
<div class="cell" data-execution_count="118">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>prediction_scorecard <span class="op">=</span> {<span class="st">'model'</span>:[],</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'accuracy_score'</span>:[],</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'precision_score'</span>:[],</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'recall_score'</span>:[],</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a>                        <span class="st">'best_params'</span>:[]}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="116">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> model_results(X_test, model, model_id):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Predict!</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>    pred <span class="op">=</span> model.predict(X_test)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    pred_proba <span class="op">=</span> model.predict_proba(X_test)</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>    df_pred <span class="op">=</span> pd.DataFrame(np.around(pred_proba, <span class="dv">4</span>), index<span class="op">=</span>X_test.index, columns<span class="op">=</span>[<span class="st">'prob_0'</span>, <span class="st">'prob_1'</span>, <span class="st">'prob_2'</span>])</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>    df_pred[<span class="st">'prediction'</span>] <span class="op">=</span> <span class="bu">list</span>(pred)</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a>    df_pred[<span class="st">'actual'</span>] <span class="op">=</span> y_test[<span class="st">'label'</span>]</span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    df_pred[<span class="st">'grid_position'</span>] <span class="op">=</span> X_test[<span class="st">'grid'</span>]</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Include row if an 'actual' or 'predicted' podium occured for calculating accuracy</span></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df_pred['sort'] = df_pred['prediction'] + df_pred['actual']</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df_pred = df_pred[df_pred['sort'] &gt; 0]</span></span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df_pred.reset_index(inplace=True)</span></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>    df_pred <span class="op">=</span> df_pred.groupby([<span class="st">'round'</span>]).<span class="bu">apply</span>(pd.DataFrame.sort_values, <span class="st">'prob_1'</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df_pred.drop(['sort'], axis=1, inplace=True)</span></span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># df_pred.reset_index(drop=True, inplace=True) </span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Save Accuracy, Precision, </span></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    prediction_scorecard[<span class="st">'model'</span>].append(model_id)</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    prediction_scorecard[<span class="st">'accuracy_score'</span>].append(accuracy_score(df_pred[<span class="st">'actual'</span>], df_pred[<span class="st">'prediction'</span>]))</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    prediction_scorecard[<span class="st">'precision_score'</span>].append(precision_score(df_pred[<span class="st">'actual'</span>], df_pred[<span class="st">'prediction'</span>], average<span class="op">=</span><span class="st">'micro'</span>))</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    prediction_scorecard[<span class="st">'recall_score'</span>].append(recall_score(df_pred[<span class="st">'actual'</span>], df_pred[<span class="st">'prediction'</span>], average<span class="op">=</span><span class="st">'micro'</span>))</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>    prediction_scorecard[<span class="st">'best_params'</span>].append(<span class="bu">str</span>(model.best_params_))</span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    display(df_pred.head(<span class="dv">40</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="grid-search-cv" class="level2">
<h2 class="anchored" data-anchor-id="grid-search-cv">Grid search CV</h2>
<ul>
<li>Hyper-parameters are variables that you specify while building a machine-learning model. This means that it’s the user that defines the hyper-parameters while building the model. Hyper-parameters control the learning process, while parameters are learned.</li>
<li>The performance of a model significantly depends on the value of hyperparameters. Note that there is no way to know in advance the best values for hyperparameters so ideally, we need to try all possible values to know the optimal values.</li>
<li>Doing this manually could take a considerable amount of time and resources and thus we use GridSearchCV to automate the tuning of hyperparameters.</li>
<li>Grid search CV of the sklearn library is a module for hyperparameter tuning.</li>
<li>It runs through all the different parameters that is fed into the parameter grid and produces the best combination of parameters, based on a scoring metric of your choice (accuracy, f1, etc).</li>
<li>GridSearchCV tries all the combinations of the values passed in the dictionary and evaluates the model for each combination using the Cross-Validation method.</li>
<li>The process is time consuming.</li>
</ul>
<div class="cell" data-execution_count="101">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>svm_params<span class="op">=</span> {<span class="st">'svm__C'</span>: [<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>],</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>             <span class="st">'svm__kernel'</span>: [<span class="st">'linear'</span>, <span class="st">'poly'</span>, <span class="st">'rbf'</span>],</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>             <span class="st">'svm__degree'</span>: [<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>],</span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>             <span class="st">'svm__gamma'</span>: [<span class="fl">0.1</span>, <span class="fl">0.01</span>, <span class="fl">0.001</span>]}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Types of SVM Kernels: <br> - <code>Linear</code>: These are commonly recommended for text classification because most of these types of classification problems are linearly separable.<br> - <code>Polynomial</code>: The polynomial kernel isn’t used in practice very often because it isn’t as computationally efficient as other kernels and its predictions aren’t as accurate.<br> - <code>Gaussian Radial Basis Function (RBF)</code>: One of the most powerful and commonly used kernels in SVMs. Usually the choice for non-linear data.</p>
<div class="cell" data-execution_count="103">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>scoring <span class="op">=</span> [<span class="st">'neg_log_loss'</span>, <span class="st">'accuracy'</span>]</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>svm_cv <span class="op">=</span> GridSearchCV(prediction_model(SVC(probability<span class="op">=</span><span class="va">True</span>), <span class="st">'svm'</span>),</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>                      param_grid<span class="op">=</span>svm_params,</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>                      scoring<span class="op">=</span>scoring, </span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>                      refit<span class="op">=</span><span class="st">'neg_log_loss'</span>,  </span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>                      verbose<span class="op">=</span><span class="dv">10</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="fitting-and-training-the-svm-model" class="level2">
<h2 class="anchored" data-anchor-id="fitting-and-training-the-svm-model">Fitting and Training the SVM model</h2>
<div class="cell" data-execution_count="104">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Train Model</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>svm_cv.fit(X_train, y_train)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Fitting 5 folds for each of 81 candidates, totalling 405 fits
[CV 1/5; 1/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 1/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.028) total time=  11.8s
[CV 2/5; 1/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 1/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.007) total time=  11.9s
[CV 3/5; 1/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 1/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.8s
[CV 4/5; 1/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 1/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.8s
[CV 5/5; 1/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 1/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.187) total time=  12.0s
[CV 1/5; 2/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 2/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.954) neg_log_loss: (test=-0.099) total time=  20.6s
[CV 2/5; 2/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 2/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  21.2s
[CV 3/5; 2/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 2/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  21.0s
[CV 4/5; 2/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 2/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.956) neg_log_loss: (test=-0.067) total time=  21.2s
[CV 5/5; 2/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 2/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.915) neg_log_loss: (test=-0.346) total time=  20.4s
[CV 1/5; 3/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf.
[CV 1/5; 3/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.860) neg_log_loss: (test=-0.304) total time=  58.2s
[CV 2/5; 3/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf.
[CV 2/5; 3/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.959) neg_log_loss: (test=-0.094) total time=  59.9s
[CV 3/5; 3/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf.
[CV 3/5; 3/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.945) neg_log_loss: (test=-0.122) total time=  58.6s
[CV 4/5; 3/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf.
[CV 4/5; 3/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.920) neg_log_loss: (test=-0.180) total time=  58.9s
[CV 5/5; 3/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf.
[CV 5/5; 3/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.822) neg_log_loss: (test=-0.444) total time=  54.3s
[CV 1/5; 4/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 4/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.028) total time=  11.5s
[CV 2/5; 4/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 4/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.008) total time=  11.8s
[CV 3/5; 4/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 4/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.8s
[CV 4/5; 4/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 4/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.7s
[CV 5/5; 4/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 4/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.188) total time=  11.8s
[CV 1/5; 5/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 5/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.910) neg_log_loss: (test=-0.259) total time=  39.5s
[CV 2/5; 5/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 5/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  40.5s
[CV 3/5; 5/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 5/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  40.1s
[CV 4/5; 5/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 5/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  40.9s
[CV 5/5; 5/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 5/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.836) neg_log_loss: (test=-0.552) total time=  38.0s
[CV 1/5; 6/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 6/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.926) neg_log_loss: (test=-0.183) total time= 1.2min
[CV 2/5; 6/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 6/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.943) neg_log_loss: (test=-0.124) total time= 1.2min
[CV 3/5; 6/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 6/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.930) neg_log_loss: (test=-0.137) total time= 1.2min
[CV 4/5; 6/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 6/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.927) neg_log_loss: (test=-0.157) total time= 1.2min
[CV 5/5; 6/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 6/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.835) neg_log_loss: (test=-0.583) total time= 1.1min
[CV 1/5; 7/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 7/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.026) total time=  11.5s
[CV 2/5; 7/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 7/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.007) total time=  11.8s
[CV 3/5; 7/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 7/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.7s
[CV 4/5; 7/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 7/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.6s
[CV 5/5; 7/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 7/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.187) total time=  11.8s
[CV 1/5; 8/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 8/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.774) neg_log_loss: (test=-0.386) total time= 1.2min
[CV 2/5; 8/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 8/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.757) neg_log_loss: (test=-0.233) total time= 1.2min
[CV 3/5; 8/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 8/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.739) neg_log_loss: (test=-0.245) total time= 1.2min
[CV 4/5; 8/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 8/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.828) neg_log_loss: (test=-0.347) total time= 1.2min
[CV 5/5; 8/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 8/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.855) neg_log_loss: (test=-0.804) total time= 1.2min
[CV 1/5; 9/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 9/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.828) neg_log_loss: (test=-0.335) total time= 2.1min
[CV 2/5; 9/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 9/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.842) neg_log_loss: (test=-0.205) total time= 2.1min
[CV 3/5; 9/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 9/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.829) neg_log_loss: (test=-0.225) total time= 2.1min
[CV 4/5; 9/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 9/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.873) neg_log_loss: (test=-0.251) total time= 2.2min
[CV 5/5; 9/81] START svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 9/81] END svm__C=0.1, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.790) neg_log_loss: (test=-0.655) total time= 2.1min
[CV 1/5; 10/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 10/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.028) total time=  11.5s
[CV 2/5; 10/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 10/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.008) total time=  11.8s
[CV 3/5; 10/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 10/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.8s
[CV 4/5; 10/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 10/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.7s
[CV 5/5; 10/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 10/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.186) total time=  11.9s
[CV 1/5; 11/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 11/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.914) neg_log_loss: (test=-0.393) total time=  25.9s
[CV 2/5; 11/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 11/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.959) neg_log_loss: (test=-0.096) total time=  26.9s
[CV 3/5; 11/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 11/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.948) neg_log_loss: (test=-0.127) total time=  26.6s
[CV 4/5; 11/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 11/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.942) neg_log_loss: (test=-0.220) total time=  26.5s
[CV 5/5; 11/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 11/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.819) neg_log_loss: (test=-0.653) total time=  23.7s
[CV 1/5; 12/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 12/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.860) neg_log_loss: (test=-0.303) total time=  58.1s
[CV 2/5; 12/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 12/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.959) neg_log_loss: (test=-0.094) total time=  59.9s
[CV 3/5; 12/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 12/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.945) neg_log_loss: (test=-0.122) total time=  58.6s
[CV 4/5; 12/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 12/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.920) neg_log_loss: (test=-0.181) total time=  59.0s
[CV 5/5; 12/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 12/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.822) neg_log_loss: (test=-0.446) total time=  54.3s
[CV 1/5; 13/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 13/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.027) total time=  11.6s
[CV 2/5; 13/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 13/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.007) total time=  11.8s
[CV 3/5; 13/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 13/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.8s
[CV 4/5; 13/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 13/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.7s
[CV 5/5; 13/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 13/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.190) total time=  11.9s
[CV 1/5; 14/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 14/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.610) neg_log_loss: (test=-0.498) total time= 1.3min
[CV 2/5; 14/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 14/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.590) neg_log_loss: (test=-0.280) total time= 1.3min
[CV 3/5; 14/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 14/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.607) neg_log_loss: (test=-0.274) total time= 1.3min
[CV 4/5; 14/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 14/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.654) neg_log_loss: (test=-0.835) total time= 1.3min
[CV 5/5; 14/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 14/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.718) neg_log_loss: (test=-0.822) total time= 1.3min
[CV 1/5; 15/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 15/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.926) neg_log_loss: (test=-0.184) total time= 1.2min
[CV 2/5; 15/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 15/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.943) neg_log_loss: (test=-0.124) total time= 1.2min
[CV 3/5; 15/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 15/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.930) neg_log_loss: (test=-0.137) total time= 1.2min
[CV 4/5; 15/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 15/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.927) neg_log_loss: (test=-0.157) total time= 1.2min
[CV 5/5; 15/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 15/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.835) neg_log_loss: (test=-0.584) total time= 1.1min
[CV 1/5; 16/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 16/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.027) total time=  11.4s
[CV 2/5; 16/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 16/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.007) total time=  11.8s
[CV 3/5; 16/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 16/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.7s
[CV 4/5; 16/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 16/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.7s
[CV 5/5; 16/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 16/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.187) total time=  11.9s
[CV 1/5; 17/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 17/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.601) total time= 1.5min
[CV 2/5; 17/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 17/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.518) total time= 1.5min
[CV 3/5; 17/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 17/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.494) total time= 1.5min
[CV 4/5; 17/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 17/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.913) total time= 1.5min
[CV 5/5; 17/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 17/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.946) total time= 1.4min
[CV 1/5; 18/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 18/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.828) neg_log_loss: (test=-0.335) total time= 2.1min
[CV 2/5; 18/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 18/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.842) neg_log_loss: (test=-0.204) total time= 2.1min
[CV 3/5; 18/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 18/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.829) neg_log_loss: (test=-0.225) total time= 2.1min
[CV 4/5; 18/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 18/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.873) neg_log_loss: (test=-0.252) total time= 2.2min
[CV 5/5; 18/81] START svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 18/81] END svm__C=0.1, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.790) neg_log_loss: (test=-0.656) total time= 2.1min
[CV 1/5; 19/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 19/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.027) total time=  11.5s
[CV 2/5; 19/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 19/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.007) total time=  11.8s
[CV 3/5; 19/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 19/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.008) total time=  11.8s
[CV 4/5; 19/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 19/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.6s
[CV 5/5; 19/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 19/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.190) total time=  11.8s
[CV 1/5; 20/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 20/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.900) neg_log_loss: (test=-0.623) total time=  26.4s
[CV 2/5; 20/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 20/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.944) neg_log_loss: (test=-0.121) total time=  26.7s
[CV 3/5; 20/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 20/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.944) neg_log_loss: (test=-0.119) total time=  26.6s
[CV 4/5; 20/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 20/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.949) neg_log_loss: (test=-0.168) total time=  27.1s
[CV 5/5; 20/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 20/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.877) neg_log_loss: (test=-0.754) total time=  24.6s
[CV 1/5; 21/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 21/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.860) neg_log_loss: (test=-0.306) total time=  58.3s
[CV 2/5; 21/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 21/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.959) neg_log_loss: (test=-0.094) total time=  59.8s
[CV 3/5; 21/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 21/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.945) neg_log_loss: (test=-0.121) total time=  58.5s
[CV 4/5; 21/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 21/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.920) neg_log_loss: (test=-0.180) total time=  59.0s
[CV 5/5; 21/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 21/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.822) neg_log_loss: (test=-0.446) total time=  54.3s
[CV 1/5; 22/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 22/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.028) total time=  11.5s
[CV 2/5; 22/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 22/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.008) total time=  11.7s
[CV 3/5; 22/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 22/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.009) total time=  11.8s
[CV 4/5; 22/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 22/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.7s
[CV 5/5; 22/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 22/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.185) total time=  11.9s
[CV 1/5; 23/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 23/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.522) total time= 1.3min
[CV 2/5; 23/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 23/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.481) total time= 1.3min
[CV 3/5; 23/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 23/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.580) neg_log_loss: (test=-0.498) total time= 1.4min
[CV 4/5; 23/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 23/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.600) neg_log_loss: (test=-0.957) total time= 1.4min
[CV 5/5; 23/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 23/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.682) neg_log_loss: (test=-0.794) total time= 1.4min
[CV 1/5; 24/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 24/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.926) neg_log_loss: (test=-0.183) total time= 1.2min
[CV 2/5; 24/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 24/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.943) neg_log_loss: (test=-0.124) total time= 1.2min
[CV 3/5; 24/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 24/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.930) neg_log_loss: (test=-0.137) total time= 1.2min
[CV 4/5; 24/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 24/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.927) neg_log_loss: (test=-0.157) total time= 1.2min
[CV 5/5; 24/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 24/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.835) neg_log_loss: (test=-0.583) total time= 1.1min
[CV 1/5; 25/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 25/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.993) neg_log_loss: (test=-0.027) total time=  11.5s
[CV 2/5; 25/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 25/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=1.000) neg_log_loss: (test=-0.007) total time=  11.8s
[CV 3/5; 25/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 25/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.999) neg_log_loss: (test=-0.008) total time=  11.7s
[CV 4/5; 25/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 25/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.994) neg_log_loss: (test=-0.030) total time=  11.6s
[CV 5/5; 25/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 25/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.960) neg_log_loss: (test=-0.190) total time=  11.8s
[CV 1/5; 26/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 26/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.863) total time= 1.3min
[CV 2/5; 26/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 26/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.903) total time= 1.3min
[CV 3/5; 26/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 26/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.866) total time= 1.3min
[CV 4/5; 26/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 26/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.936) total time= 1.3min
[CV 5/5; 26/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 26/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.798) total time= 1.3min
[CV 1/5; 27/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 27/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.828) neg_log_loss: (test=-0.335) total time= 2.1min
[CV 2/5; 27/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 27/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.842) neg_log_loss: (test=-0.205) total time= 2.1min
[CV 3/5; 27/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 27/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.829) neg_log_loss: (test=-0.225) total time= 2.1min
[CV 4/5; 27/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 27/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.873) neg_log_loss: (test=-0.252) total time= 2.2min
[CV 5/5; 27/81] START svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 27/81] END svm__C=0.1, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.790) neg_log_loss: (test=-0.656) total time= 2.1min
[CV 1/5; 28/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 28/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  20.3s
[CV 2/5; 28/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 28/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  20.8s
[CV 3/5; 28/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 28/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  20.2s
[CV 4/5; 28/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 28/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  20.9s
[CV 5/5; 28/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 28/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.348) total time=  20.3s
[CV 1/5; 29/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 29/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.910) neg_log_loss: (test=-0.259) total time=  39.4s
[CV 2/5; 29/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 29/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  40.4s
[CV 3/5; 29/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 29/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  40.1s
[CV 4/5; 29/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 29/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  40.8s
[CV 5/5; 29/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 29/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.836) neg_log_loss: (test=-0.554) total time=  38.0s
[CV 1/5; 30/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 30/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.763) neg_log_loss: (test=-0.381) total time= 1.9min
[CV 2/5; 30/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 30/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.909) neg_log_loss: (test=-0.182) total time= 2.0min
[CV 3/5; 30/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 30/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.875) neg_log_loss: (test=-0.167) total time= 1.9min
[CV 4/5; 30/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 30/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.878) neg_log_loss: (test=-0.244) total time= 1.9min
[CV 5/5; 30/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 30/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.737) neg_log_loss: (test=-0.608) total time= 1.8min
[CV 1/5; 31/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 31/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  20.3s
[CV 2/5; 31/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 31/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  20.9s
[CV 3/5; 31/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 31/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  20.4s
[CV 4/5; 31/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 31/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  20.8s
[CV 5/5; 31/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 31/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.345) total time=  20.1s
[CV 1/5; 32/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 32/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.774) neg_log_loss: (test=-0.386) total time= 1.2min
[CV 2/5; 32/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 32/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.757) neg_log_loss: (test=-0.232) total time= 1.2min
[CV 3/5; 32/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 32/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.739) neg_log_loss: (test=-0.245) total time= 1.2min
[CV 4/5; 32/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 32/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.828) neg_log_loss: (test=-0.347) total time= 1.2min
[CV 5/5; 32/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 32/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.855) neg_log_loss: (test=-0.803) total time= 1.2min
[CV 1/5; 33/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 33/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.811) neg_log_loss: (test=-0.327) total time= 2.2min
[CV 2/5; 33/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 33/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.830) neg_log_loss: (test=-0.219) total time= 2.3min
[CV 3/5; 33/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 33/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.817) neg_log_loss: (test=-0.245) total time= 2.3min
[CV 4/5; 33/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 33/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.853) neg_log_loss: (test=-0.303) total time= 2.0min
[CV 5/5; 33/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 33/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.783) neg_log_loss: (test=-0.656) total time= 1.7min
[CV 1/5; 34/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 34/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  15.3s
[CV 2/5; 34/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 34/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  15.9s
[CV 3/5; 34/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 34/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.5s
[CV 4/5; 34/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 34/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 5/5; 34/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 34/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.347) total time=  15.3s
[CV 1/5; 35/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 35/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.435) total time= 1.0min
[CV 2/5; 35/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 35/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.258) total time= 1.0min
[CV 3/5; 35/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 35/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.282) total time= 1.0min
[CV 4/5; 35/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 35/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.572) total time= 1.0min
[CV 5/5; 35/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 35/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.995) total time= 1.0min
[CV 1/5; 36/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 36/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.429) total time= 2.0min
[CV 2/5; 36/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 36/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.256) total time= 2.0min
[CV 3/5; 36/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 36/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.280) total time= 2.0min
[CV 4/5; 36/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 36/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.570) total time= 2.0min
[CV 5/5; 36/81] START svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 36/81] END svm__C=0.01, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.990) total time= 2.0min
[CV 1/5; 37/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 37/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  15.4s
[CV 2/5; 37/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 37/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 3/5; 37/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 37/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.6s
[CV 4/5; 37/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 37/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 5/5; 37/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 37/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.346) total time=  15.4s
[CV 1/5; 38/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 38/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.872) neg_log_loss: (test=-0.383) total time=  39.0s
[CV 2/5; 38/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 38/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.905) neg_log_loss: (test=-0.190) total time=  39.6s
[CV 3/5; 38/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 38/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.878) neg_log_loss: (test=-0.195) total time=  39.1s
[CV 4/5; 38/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 38/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.905) neg_log_loss: (test=-0.282) total time=  39.0s
[CV 5/5; 38/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 38/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.824) neg_log_loss: (test=-0.877) total time=  35.8s
[CV 1/5; 39/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 39/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.763) neg_log_loss: (test=-0.382) total time= 1.4min
[CV 2/5; 39/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 39/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.909) neg_log_loss: (test=-0.182) total time= 1.5min
[CV 3/5; 39/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 39/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.875) neg_log_loss: (test=-0.167) total time= 1.5min
[CV 4/5; 39/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 39/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.878) neg_log_loss: (test=-0.244) total time= 1.5min
[CV 5/5; 39/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 39/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.737) neg_log_loss: (test=-0.607) total time= 1.4min
[CV 1/5; 40/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 40/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  15.7s
[CV 2/5; 40/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 40/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  15.9s
[CV 3/5; 40/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 40/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.6s
[CV 4/5; 40/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 40/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 5/5; 40/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 40/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.347) total time=  15.4s
[CV 1/5; 41/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 41/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.519) total time= 1.1min
[CV 2/5; 41/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 41/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.447) total time= 1.1min
[CV 3/5; 41/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 41/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.422) total time= 1.1min
[CV 4/5; 41/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 41/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.574) neg_log_loss: (test=-0.840) total time= 1.1min
[CV 5/5; 41/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 41/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.636) neg_log_loss: (test=-0.830) total time= 1.1min
[CV 1/5; 42/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 42/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.811) neg_log_loss: (test=-0.327) total time= 1.7min
[CV 2/5; 42/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 42/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.830) neg_log_loss: (test=-0.220) total time= 1.7min
[CV 3/5; 42/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 42/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.817) neg_log_loss: (test=-0.245) total time= 1.7min
[CV 4/5; 42/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 42/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.853) neg_log_loss: (test=-0.303) total time= 1.8min
[CV 5/5; 42/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 42/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.783) neg_log_loss: (test=-0.655) total time= 1.7min
[CV 1/5; 43/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 43/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  15.5s
[CV 2/5; 43/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 43/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.068) total time=  16.0s
[CV 3/5; 43/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 43/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.6s
[CV 4/5; 43/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 43/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 5/5; 43/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 43/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.347) total time=  15.5s
[CV 1/5; 44/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 44/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.864) total time= 1.0min
[CV 2/5; 44/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 44/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.012) total time= 1.0min
[CV 3/5; 44/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 44/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.816) total time= 1.0min
[CV 4/5; 44/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 44/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.027) total time= 1.0min
[CV 5/5; 44/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 44/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.509) total time= 1.0min
[CV 1/5; 45/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 45/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.429) total time= 2.0min
[CV 2/5; 45/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 45/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.256) total time= 2.0min
[CV 3/5; 45/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 45/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.280) total time= 2.0min
[CV 4/5; 45/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 45/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.573) total time= 2.0min
[CV 5/5; 45/81] START svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 45/81] END svm__C=0.01, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.989) total time= 2.0min
[CV 1/5; 46/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 46/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.099) total time=  15.5s
[CV 2/5; 46/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 46/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 3/5; 46/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 46/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.6s
[CV 4/5; 46/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 46/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 5/5; 46/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 46/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.346) total time=  15.5s
[CV 1/5; 47/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 47/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.884) neg_log_loss: (test=-0.547) total time=  38.9s
[CV 2/5; 47/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 47/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.877) neg_log_loss: (test=-0.196) total time=  38.9s
[CV 3/5; 47/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 47/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.844) neg_log_loss: (test=-0.220) total time=  38.5s
[CV 4/5; 47/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 47/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.903) neg_log_loss: (test=-0.289) total time=  39.5s
[CV 5/5; 47/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 47/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.840) neg_log_loss: (test=-0.964) total time=  36.7s
[CV 1/5; 48/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 48/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.763) neg_log_loss: (test=-0.382) total time= 1.4min
[CV 2/5; 48/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 48/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.909) neg_log_loss: (test=-0.182) total time= 1.5min
[CV 3/5; 48/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 48/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.875) neg_log_loss: (test=-0.167) total time= 1.5min
[CV 4/5; 48/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 48/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.878) neg_log_loss: (test=-0.245) total time= 2.1min
[CV 5/5; 48/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 48/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.737) neg_log_loss: (test=-0.608) total time= 1.4min
[CV 1/5; 49/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 49/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  15.4s
[CV 2/5; 49/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 49/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  15.6s
[CV 3/5; 49/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 49/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.4s
[CV 4/5; 49/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 49/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  15.8s
[CV 5/5; 49/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 49/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.347) total time=  15.2s
[CV 1/5; 50/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 50/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.720) total time= 1.1min
[CV 2/5; 50/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 50/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.817) total time= 1.1min
[CV 3/5; 50/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 50/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.681) total time= 1.1min
[CV 4/5; 50/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 50/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.572) neg_log_loss: (test=-0.842) total time= 1.1min
[CV 5/5; 50/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 50/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.618) neg_log_loss: (test=-0.806) total time= 1.1min
[CV 1/5; 51/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 51/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.811) neg_log_loss: (test=-0.327) total time= 1.8min
[CV 2/5; 51/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 51/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.830) neg_log_loss: (test=-0.220) total time= 2.7min
[CV 3/5; 51/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 51/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.817) neg_log_loss: (test=-0.245) total time= 1.7min
[CV 4/5; 51/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 51/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.853) neg_log_loss: (test=-0.303) total time= 1.8min
[CV 5/5; 51/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 51/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.783) neg_log_loss: (test=-0.655) total time= 1.7min
[CV 1/5; 52/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 52/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.954) neg_log_loss: (test=-0.100) total time=  15.4s
[CV 2/5; 52/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 52/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.976) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 3/5; 52/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 52/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.971) neg_log_loss: (test=-0.059) total time=  15.6s
[CV 4/5; 52/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 52/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.962) neg_log_loss: (test=-0.067) total time=  16.0s
[CV 5/5; 52/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 52/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.915) neg_log_loss: (test=-0.344) total time=  15.5s
[CV 1/5; 53/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 53/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.866) total time= 1.0min
[CV 2/5; 53/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 53/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.901) total time= 1.0min
[CV 3/5; 53/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 53/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.106) total time= 1.1min
[CV 4/5; 53/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 53/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.939) total time= 2.9min
[CV 5/5; 53/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 53/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.714) total time= 8.2min
[CV 1/5; 54/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 54/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.429) total time= 2.0min
[CV 2/5; 54/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 54/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.256) total time= 2.0min
[CV 3/5; 54/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 54/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.280) total time= 2.0min
[CV 4/5; 54/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 54/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.571) total time= 2.0min
[CV 5/5; 54/81] START svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 54/81] END svm__C=0.01, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.986) total time= 2.0min
[CV 1/5; 55/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 55/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.260) total time=  31.3s
[CV 2/5; 55/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 55/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  32.8s
[CV 3/5; 55/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 55/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  31.8s
[CV 4/5; 55/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 55/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  32.5s
[CV 5/5; 55/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 55/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.553) total time=  31.4s
[CV 1/5; 56/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 56/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.774) neg_log_loss: (test=-0.386) total time=  55.1s
[CV 2/5; 56/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 56/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.757) neg_log_loss: (test=-0.232) total time=  55.0s
[CV 3/5; 56/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 56/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.739) neg_log_loss: (test=-0.245) total time=  55.3s
[CV 4/5; 56/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 56/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.828) neg_log_loss: (test=-0.347) total time=  55.1s
[CV 5/5; 56/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 56/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.855) neg_log_loss: (test=-0.804) total time=  54.7s
[CV 1/5; 57/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 57/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.515) total time= 2.0min
[CV 2/5; 57/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 57/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.246) total time= 2.0min
[CV 3/5; 57/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 57/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.580) neg_log_loss: (test=-0.182) total time= 2.0min
[CV 4/5; 57/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 57/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.358) total time= 2.0min
[CV 5/5; 57/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 57/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-1.032) total time= 2.0min
[CV 1/5; 58/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 58/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.259) total time=  32.2s
[CV 2/5; 58/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 58/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  32.1s
[CV 3/5; 58/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 58/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  32.6s
[CV 4/5; 58/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 58/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  33.2s
[CV 5/5; 58/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 58/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.552) total time=  31.3s
[CV 1/5; 59/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 59/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.435) total time= 1.0min
[CV 2/5; 59/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 59/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.259) total time= 1.0min
[CV 3/5; 59/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 59/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.282) total time= 1.0min
[CV 4/5; 59/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 59/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.572) total time= 1.0min
[CV 5/5; 59/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 59/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.995) total time= 1.0min
[CV 1/5; 60/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 60/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.406) total time= 2.0min
[CV 2/5; 60/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 60/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.243) total time= 2.0min
[CV 3/5; 60/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 60/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.257) total time= 2.1min
[CV 4/5; 60/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 60/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.525) total time= 2.1min
[CV 5/5; 60/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 60/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-1.013) total time= 2.1min
[CV 1/5; 61/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 61/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.259) total time=  31.4s
[CV 2/5; 61/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 61/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  32.3s
[CV 3/5; 61/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 61/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  33.2s
[CV 4/5; 61/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 61/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  33.5s
[CV 5/5; 61/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 61/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.552) total time=  31.1s
[CV 1/5; 62/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 62/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.463) total time= 1.0min
[CV 2/5; 62/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 62/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.267) total time= 1.0min
[CV 3/5; 62/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 62/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.281) total time= 1.0min
[CV 4/5; 62/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 62/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.598) total time= 1.0min
[CV 5/5; 62/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 62/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.981) total time=  59.9s
[CV 1/5; 63/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 63/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.442) total time= 2.0min
[CV 2/5; 63/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 63/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.261) total time= 2.0min
[CV 3/5; 63/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 63/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.280) total time= 2.0min
[CV 4/5; 63/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 63/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.586) total time= 2.0min
[CV 5/5; 63/81] START svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 63/81] END svm__C=0.001, svm__degree=1, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.985) total time= 2.0min
[CV 1/5; 64/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 64/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.260) total time=  31.9s
[CV 2/5; 64/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 64/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  33.2s
[CV 3/5; 64/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 64/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  32.0s
[CV 4/5; 64/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 64/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  32.5s
[CV 5/5; 64/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 64/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.552) total time=  30.5s
[CV 1/5; 65/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 65/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.610) neg_log_loss: (test=-0.500) total time=  59.6s
[CV 2/5; 65/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 65/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.590) neg_log_loss: (test=-0.280) total time= 1.0min
[CV 3/5; 65/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 65/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.607) neg_log_loss: (test=-0.275) total time= 1.0min
[CV 4/5; 65/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 65/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.654) neg_log_loss: (test=-0.833) total time= 1.0min
[CV 5/5; 65/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 65/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.718) neg_log_loss: (test=-0.822) total time=  59.0s
[CV 1/5; 66/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 66/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.517) total time= 2.0min
[CV 2/5; 66/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 66/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.246) total time= 2.0min
[CV 3/5; 66/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 66/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.580) neg_log_loss: (test=-0.182) total time= 2.0min
[CV 4/5; 66/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 66/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.357) total time= 2.0min
[CV 5/5; 66/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 66/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-1.033) total time= 2.0min
[CV 1/5; 67/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 67/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.259) total time=  35.2s
[CV 2/5; 67/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 67/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  31.7s
[CV 3/5; 67/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 67/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  30.7s
[CV 4/5; 67/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 67/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  33.2s
[CV 5/5; 67/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 67/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.549) total time=  31.6s
[CV 1/5; 68/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 68/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.605) total time= 1.1min
[CV 2/5; 68/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 68/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.524) total time= 1.1min
[CV 3/5; 68/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 68/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.485) total time= 1.1min
[CV 4/5; 68/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 68/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.914) total time= 1.2min
[CV 5/5; 68/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 68/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.939) total time= 1.1min
[CV 1/5; 69/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 69/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.406) total time= 2.0min
[CV 2/5; 69/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 69/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.243) total time= 2.0min
[CV 3/5; 69/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 69/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.257) total time= 2.0min
[CV 4/5; 69/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 69/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.525) total time= 2.0min
[CV 5/5; 69/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 69/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-1.016) total time= 2.0min
[CV 1/5; 70/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 70/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.260) total time=  31.8s
[CV 2/5; 70/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 70/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  32.8s
[CV 3/5; 70/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 70/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  32.6s
[CV 4/5; 70/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 70/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  32.5s
[CV 5/5; 70/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 70/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.553) total time=  30.6s
[CV 1/5; 71/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 71/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.854) total time= 1.2min
[CV 2/5; 71/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 71/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.019) total time= 1.3min
[CV 3/5; 71/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 71/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-2.012) total time= 1.3min
[CV 4/5; 71/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 71/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.019) total time= 1.4min
[CV 5/5; 71/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 71/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.410) total time= 1.3min
[CV 1/5; 72/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 72/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.441) total time= 2.7min
[CV 2/5; 72/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 72/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.261) total time= 2.7min
[CV 3/5; 72/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 72/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.280) total time= 2.7min
[CV 4/5; 72/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 72/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.585) total time= 2.7min
[CV 5/5; 72/81] START svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 72/81] END svm__C=0.001, svm__degree=2, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.987) total time= 2.7min
[CV 1/5; 73/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 1/5; 73/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.259) total time=  42.1s
[CV 2/5; 73/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 2/5; 73/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  43.8s
[CV 3/5; 73/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 3/5; 73/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  42.9s
[CV 4/5; 73/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 4/5; 73/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  43.6s
[CV 5/5; 73/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear
[CV 5/5; 73/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.553) total time=  40.2s
[CV 1/5; 74/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 1/5; 74/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.695) neg_log_loss: (test=-0.622) total time= 1.2min
[CV 2/5; 74/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 2/5; 74/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.590) neg_log_loss: (test=-0.311) total time= 1.2min
[CV 3/5; 74/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 3/5; 74/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.612) neg_log_loss: (test=-0.367) total time= 1.2min
[CV 4/5; 74/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 4/5; 74/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.671) neg_log_loss: (test=-0.783) total time= 1.2min
[CV 5/5; 74/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly
[CV 5/5; 74/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=poly; accuracy: (test=0.729) neg_log_loss: (test=-1.091) total time= 1.2min
[CV 1/5; 75/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 1/5; 75/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.517) total time= 2.6min
[CV 2/5; 75/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 2/5; 75/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.245) total time= 2.6min
[CV 3/5; 75/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 3/5; 75/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.580) neg_log_loss: (test=-0.182) total time= 2.7min
[CV 4/5; 75/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 4/5; 75/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.358) total time= 2.7min
[CV 5/5; 75/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf
[CV 5/5; 75/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.1, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-1.031) total time= 2.7min
[CV 1/5; 76/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 1/5; 76/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.260) total time=  42.5s
[CV 2/5; 76/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 2/5; 76/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  43.7s
[CV 3/5; 76/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 3/5; 76/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  43.1s
[CV 4/5; 76/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 4/5; 76/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  44.0s
[CV 5/5; 76/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear
[CV 5/5; 76/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.553) total time=  41.2s
[CV 1/5; 77/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 1/5; 77/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.821) total time= 1.3min
[CV 2/5; 77/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 2/5; 77/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.886) total time= 1.3min
[CV 3/5; 77/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 3/5; 77/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.766) total time= 1.4min
[CV 4/5; 77/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 4/5; 77/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.935) total time= 1.4min
[CV 5/5; 77/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly
[CV 5/5; 77/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.817) total time= 1.3min
[CV 1/5; 78/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 1/5; 78/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.405) total time= 2.6min
[CV 2/5; 78/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 2/5; 78/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.243) total time= 2.7min
[CV 3/5; 78/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 3/5; 78/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.257) total time= 2.7min
[CV 4/5; 78/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 4/5; 78/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.524) total time= 2.7min
[CV 5/5; 78/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf
[CV 5/5; 78/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.01, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-1.013) total time= 2.7min
[CV 1/5; 79/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 1/5; 79/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.910) neg_log_loss: (test=-0.260) total time=  43.2s
[CV 2/5; 79/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 2/5; 79/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.931) neg_log_loss: (test=-0.143) total time=  43.7s
[CV 3/5; 79/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 3/5; 79/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.920) neg_log_loss: (test=-0.142) total time=  42.0s
[CV 4/5; 79/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 4/5; 79/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.929) neg_log_loss: (test=-0.152) total time=  44.0s
[CV 5/5; 79/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear
[CV 5/5; 79/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=linear; accuracy: (test=0.836) neg_log_loss: (test=-0.552) total time=  40.1s
[CV 1/5; 80/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 1/5; 80/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.855) total time= 1.3min
[CV 2/5; 80/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 2/5; 80/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.907) total time= 1.3min
[CV 3/5; 80/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 3/5; 80/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-1.150) total time= 1.3min
[CV 4/5; 80/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 4/5; 80/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.937) total time= 1.3min
[CV 5/5; 80/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly
[CV 5/5; 80/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=poly; accuracy: (test=0.578) neg_log_loss: (test=-0.733) total time= 1.3min
[CV 1/5; 81/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 1/5; 81/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.441) total time= 2.6min
[CV 2/5; 81/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 2/5; 81/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.261) total time= 2.7min
[CV 3/5; 81/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 3/5; 81/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.280) total time= 2.7min
[CV 4/5; 81/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 4/5; 81/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.585) total time= 2.7min
[CV 5/5; 81/81] START svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf
[CV 5/5; 81/81] END svm__C=0.001, svm__degree=3, svm__gamma=0.001, svm__kernel=rbf; accuracy: (test=0.578) neg_log_loss: (test=-0.985) total time= 2.7min</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="104">
<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: "▸";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: "▾";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: "";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: "";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id="sk-container-id-1" class="sk-top-container"><div class="sk-text-repr-fallback"><pre>GridSearchCV(estimator=Pipeline(steps=[('prep',
                                        ColumnTransformer(transformers=[('num',
                                                                         Pipeline(steps=[('scaler',
                                                                                          StandardScaler())]),
                                                                         ['circuitId',
                                                                          'position',
                                                                          'points',
                                                                          'grid',
                                                                          'laps',
                                                                          'age_on_race',
                                                                          'cumulative_points',
                                                                          'cumulative_laps',
                                                                          'pole_driverId',
                                                                          'pole_history',
                                                                          'win_driverId',
                                                                          'win_history']),
                                                                        ('cat',
                                                                         Pipeline(steps=[('ohe',
                                                                                          OneHotEncoder(handle_unknown='ignore'))]),
                                                                         ['status',
                                                                          'weather',
                                                                          'stop'])])),
                                       ('svm', SVC(probability=True))]),
             param_grid={'svm__C': [0.1, 0.01, 0.001], 'svm__degree': [1, 2, 3],
                         'svm__gamma': [0.1, 0.01, 0.001],
                         'svm__kernel': ['linear', 'poly', 'rbf']},
             refit='neg_log_loss', scoring=['neg_log_loss', 'accuracy'],
             verbose=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br>On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class="sk-container" hidden=""><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-1" type="checkbox"><label for="sk-estimator-id-1" class="sk-toggleable__label sk-toggleable__label-arrow">GridSearchCV</label><div class="sk-toggleable__content"><pre>GridSearchCV(estimator=Pipeline(steps=[('prep',
                                        ColumnTransformer(transformers=[('num',
                                                                         Pipeline(steps=[('scaler',
                                                                                          StandardScaler())]),
                                                                         ['circuitId',
                                                                          'position',
                                                                          'points',
                                                                          'grid',
                                                                          'laps',
                                                                          'age_on_race',
                                                                          'cumulative_points',
                                                                          'cumulative_laps',
                                                                          'pole_driverId',
                                                                          'pole_history',
                                                                          'win_driverId',
                                                                          'win_history']),
                                                                        ('cat',
                                                                         Pipeline(steps=[('ohe',
                                                                                          OneHotEncoder(handle_unknown='ignore'))]),
                                                                         ['status',
                                                                          'weather',
                                                                          'stop'])])),
                                       ('svm', SVC(probability=True))]),
             param_grid={'svm__C': [0.1, 0.01, 0.001], 'svm__degree': [1, 2, 3],
                         'svm__gamma': [0.1, 0.01, 0.001],
                         'svm__kernel': ['linear', 'poly', 'rbf']},
             refit='neg_log_loss', scoring=['neg_log_loss', 'accuracy'],
             verbose=10)</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-2" type="checkbox"><label for="sk-estimator-id-2" class="sk-toggleable__label sk-toggleable__label-arrow">estimator: Pipeline</label><div class="sk-toggleable__content"><pre>Pipeline(steps=[('prep',
                 ColumnTransformer(transformers=[('num',
                                                  Pipeline(steps=[('scaler',
                                                                   StandardScaler())]),
                                                  ['circuitId', 'position',
                                                   'points', 'grid', 'laps',
                                                   'age_on_race',
                                                   'cumulative_points',
                                                   'cumulative_laps',
                                                   'pole_driverId',
                                                   'pole_history',
                                                   'win_driverId',
                                                   'win_history']),
                                                 ('cat',
                                                  Pipeline(steps=[('ohe',
                                                                   OneHotEncoder(handle_unknown='ignore'))]),
                                                  ['status', 'weather',
                                                   'stop'])])),
                ('svm', SVC(probability=True))])</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item sk-dashed-wrapped"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-3" type="checkbox"><label for="sk-estimator-id-3" class="sk-toggleable__label sk-toggleable__label-arrow">prep: ColumnTransformer</label><div class="sk-toggleable__content"><pre>ColumnTransformer(transformers=[('num',
                                 Pipeline(steps=[('scaler', StandardScaler())]),
                                 ['circuitId', 'position', 'points', 'grid',
                                  'laps', 'age_on_race', 'cumulative_points',
                                  'cumulative_laps', 'pole_driverId',
                                  'pole_history', 'win_driverId',
                                  'win_history']),
                                ('cat',
                                 Pipeline(steps=[('ohe',
                                                  OneHotEncoder(handle_unknown='ignore'))]),
                                 ['status', 'weather', 'stop'])])</pre></div></div></div><div class="sk-parallel"><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-4" type="checkbox"><label for="sk-estimator-id-4" class="sk-toggleable__label sk-toggleable__label-arrow">num</label><div class="sk-toggleable__content"><pre>['circuitId', 'position', 'points', 'grid', 'laps', 'age_on_race', 'cumulative_points', 'cumulative_laps', 'pole_driverId', 'pole_history', 'win_driverId', 'win_history']</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-5" type="checkbox"><label for="sk-estimator-id-5" class="sk-toggleable__label sk-toggleable__label-arrow">StandardScaler</label><div class="sk-toggleable__content"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class="sk-parallel-item"><div class="sk-item"><div class="sk-label-container"><div class="sk-label sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-6" type="checkbox"><label for="sk-estimator-id-6" class="sk-toggleable__label sk-toggleable__label-arrow">cat</label><div class="sk-toggleable__content"><pre>['status', 'weather', 'stop']</pre></div></div></div><div class="sk-serial"><div class="sk-item"><div class="sk-serial"><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-7" type="checkbox"><label for="sk-estimator-id-7" class="sk-toggleable__label sk-toggleable__label-arrow">OneHotEncoder</label><div class="sk-toggleable__content"><pre>OneHotEncoder(handle_unknown='ignore')</pre></div></div></div></div></div></div></div></div></div></div><div class="sk-item"><div class="sk-estimator sk-toggleable"><input class="sk-toggleable__control sk-hidden--visually" id="sk-estimator-id-8" type="checkbox"><label for="sk-estimator-id-8" class="sk-toggleable__label sk-toggleable__label-arrow">SVC</label><div class="sk-toggleable__content"><pre>SVC(probability=True)</pre></div></div></div></div></div></div></div></div></div></div></div></div>
</div>
</div>
</section>
<section id="testing-the-svm-model" class="level2">
<h2 class="anchored" data-anchor-id="testing-the-svm-model">Testing the SVM model</h2>
<div class="cell" data-execution_count="119">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Test Model</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>model_results(X_test, svm_cv, <span class="st">'Support Vector Machines'</span>)</span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>svm_results <span class="op">=</span> pd.DataFrame(prediction_scorecard)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th>prob_0</th>
      <th>prob_1</th>
      <th>prob_2</th>
      <th>prediction</th>
      <th>actual</th>
      <th>grid_position</th>
    </tr>
    <tr>
      <th>round</th>
      <th>season</th>
      <th>round</th>
      <th>driverRef</th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="20" valign="top">1</th>
      <th rowspan="20" valign="top">2021</th>
      <th rowspan="20" valign="top">1</th>
      <th>hamilton</th>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>Podium</td>
      <td>Podium</td>
      <td>2</td>
    </tr>
    <tr>
      <th>max_verstappen</th>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>Podium</td>
      <td>Podium</td>
      <td>1</td>
    </tr>
    <tr>
      <th>bottas</th>
      <td>0.0000</td>
      <td>0.9989</td>
      <td>0.0011</td>
      <td>Podium</td>
      <td>Podium</td>
      <td>3</td>
    </tr>
    <tr>
      <th>norris</th>
      <td>0.0001</td>
      <td>0.0109</td>
      <td>0.9891</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>7</td>
    </tr>
    <tr>
      <th>raikkonen</th>
      <td>0.9990</td>
      <td>0.0009</td>
      <td>0.0001</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>14</td>
    </tr>
    <tr>
      <th>giovinazzi</th>
      <td>0.9998</td>
      <td>0.0002</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>12</td>
    </tr>
    <tr>
      <th>ocon</th>
      <td>0.9999</td>
      <td>0.0001</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>16</td>
    </tr>
    <tr>
      <th>ricciardo</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>6</td>
    </tr>
    <tr>
      <th>sainz</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>8</td>
    </tr>
    <tr>
      <th>perez</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>0</td>
    </tr>
    <tr>
      <th>alonso</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>9</td>
    </tr>
    <tr>
      <th>stroll</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>gasly</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>5</td>
    </tr>
    <tr>
      <th>leclerc</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>4</td>
    </tr>
    <tr>
      <th>vettel</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>20</td>
    </tr>
    <tr>
      <th>russell</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>15</td>
    </tr>
    <tr>
      <th>latifi</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>17</td>
    </tr>
    <tr>
      <th>tsunoda</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>13</td>
    </tr>
    <tr>
      <th>mick_schumacher</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>18</td>
    </tr>
    <tr>
      <th>mazepin</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>19</td>
    </tr>
    <tr>
      <th rowspan="20" valign="top">2</th>
      <th rowspan="20" valign="top">2021</th>
      <th rowspan="20" valign="top">2</th>
      <th>hamilton</th>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>Podium</td>
      <td>Podium</td>
      <td>1</td>
    </tr>
    <tr>
      <th>max_verstappen</th>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>Podium</td>
      <td>Podium</td>
      <td>3</td>
    </tr>
    <tr>
      <th>norris</th>
      <td>0.0000</td>
      <td>0.9954</td>
      <td>0.0046</td>
      <td>Podium</td>
      <td>Podium</td>
      <td>7</td>
    </tr>
    <tr>
      <th>leclerc</th>
      <td>0.0001</td>
      <td>0.0131</td>
      <td>0.9868</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>4</td>
    </tr>
    <tr>
      <th>perez</th>
      <td>0.9988</td>
      <td>0.0011</td>
      <td>0.0001</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>2</td>
    </tr>
    <tr>
      <th>tsunoda</th>
      <td>0.9997</td>
      <td>0.0003</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>20</td>
    </tr>
    <tr>
      <th>raikkonen</th>
      <td>0.9999</td>
      <td>0.0001</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>16</td>
    </tr>
    <tr>
      <th>gasly</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>5</td>
    </tr>
    <tr>
      <th>mick_schumacher</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>18</td>
    </tr>
    <tr>
      <th>latifi</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>14</td>
    </tr>
    <tr>
      <th>russell</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>12</td>
    </tr>
    <tr>
      <th>giovinazzi</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>17</td>
    </tr>
    <tr>
      <th>stroll</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>10</td>
    </tr>
    <tr>
      <th>alonso</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>15</td>
    </tr>
    <tr>
      <th>ocon</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>9</td>
    </tr>
    <tr>
      <th>sainz</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>11</td>
    </tr>
    <tr>
      <th>bottas</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>8</td>
    </tr>
    <tr>
      <th>ricciardo</th>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>1.0000</td>
      <td>Top_10</td>
      <td>Top_10</td>
      <td>6</td>
    </tr>
    <tr>
      <th>vettel</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>0</td>
    </tr>
    <tr>
      <th>mazepin</th>
      <td>1.0000</td>
      <td>0.0000</td>
      <td>0.0000</td>
      <td>Outside_Top_10</td>
      <td>Outside_Top_10</td>
      <td>19</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
<p>The best parameters for our model are:</p>
<div class="cell" data-execution_count="122">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>svm_cv.best_params_</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="122">
<pre><code>{'svm__C': 0.1, 'svm__degree': 3, 'svm__gamma': 0.01, 'svm__kernel': 'linear'}</code></pre>
</div>
</div>
<div class="cell" data-execution_count="123">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>svm_results</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="123">

<div>

<table class="dataframe table table-sm table-striped">
  <thead>
    <tr>
      <th></th>
      <th>model</th>
      <th>accuracy_score</th>
      <th>precision_score</th>
      <th>recall_score</th>
      <th>best_params</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>Support Vector Machines</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>1.0</td>
      <td>{'svm__C': 0.1, 'svm__degree': 3, 'svm__gamma'...</td>
    </tr>
  </tbody>
</table>
</div>
</div>
</div>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<ul>
<li>The SVM model gives out 100% accuracy, precision and recall values.</li>
<li>The ideal hyperparameters are:
<ul>
<li>C = 0.1</li>
<li>degree = 3</li>
<li>gamma = 0.01</li>
<li>kernel = linear</li>
</ul></li>
</ul>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>